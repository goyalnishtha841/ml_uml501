{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78b8eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# ------------ Load dataset ------------\n",
    "df = pd.read_csv(\"spam.csv\", encoding='latin-1')[[\"v1\", \"v2\"]]\n",
    "df.columns = [\"label\", \"text\"]\n",
    "\n",
    "# ------------ Convert label spam=1, ham=0 ------------\n",
    "df['label'] = df['label'].map({'spam': 1, 'ham': 0})\n",
    "\n",
    "# ------------ Text cleaning ------------\n",
    "def clean_text(t):\n",
    "    t = t.lower()\n",
    "    t = re.sub(r'[^\\w\\s]', '', t)  # remove punctuation\n",
    "    return t\n",
    "\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# ------------ TF-IDF Vectorization ------------\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "X = tfidf.fit_transform(df['text'])\n",
    "y = df['label']\n",
    "\n",
    "# ------------ Train–test split ------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ------------ Class distribution ------------\n",
    "print(\"Class distribution:\")\n",
    "print(df['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b45532",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "stump = DecisionTreeClassifier(max_depth=1)\n",
    "stump.fit(X_train, y_train)\n",
    "\n",
    "train_acc = stump.score(X_train, y_train)\n",
    "test_acc = stump.score(X_test, y_test)\n",
    "\n",
    "print(\"\\n=== Decision Stump Performance ===\")\n",
    "print(\"Train Accuracy:\", train_acc)\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "\n",
    "y_pred = stump.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "print(\"\\nWhy stump performs poorly?\")\n",
    "print(\"Because text data is high-dimensional & nonlinear; a depth-1 stump cannot split complex patterns.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536cc925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "T = 15\n",
    "n = X_train.shape[0]\n",
    "weights = np.ones(n) / n\n",
    "\n",
    "alphas = []\n",
    "errors = []\n",
    "\n",
    "for t in range(T):\n",
    "\n",
    "    stump = DecisionTreeClassifier(max_depth=1)\n",
    "    stump.fit(X_train, y_train, sample_weight=weights)\n",
    "    pred = stump.predict(X_train)\n",
    "\n",
    "    # weighted error\n",
    "    err = np.sum(weights * (pred != y_train))\n",
    "\n",
    "    # avoid division by zero\n",
    "    err = max(err, 1e-10)\n",
    "\n",
    "    alpha = 0.5 * np.log((1 - err) / err)\n",
    "\n",
    "    # Print iteration output\n",
    "    print(f\"\\n=== Iteration {t+1} ===\")\n",
    "    print(\"Weighted error:\", err)\n",
    "    print(\"Alpha:\", alpha)\n",
    "\n",
    "    misclassified_idx = np.where(pred != y_train)[0]\n",
    "    print(\"Misclassified sample indices:\", misclassified_idx)\n",
    "    print(\"Weights of misclassified samples:\", weights[misclassified_idx][:10])\n",
    "\n",
    "    # update weights\n",
    "    weights *= np.exp(-alpha * y_train.replace({0:-1}) * pred)\n",
    "    weights /= np.sum(weights)\n",
    "\n",
    "    alphas.append(alpha)\n",
    "    errors.append(err)\n",
    "\n",
    "# ----- Final boosted model prediction -----\n",
    "def predict_boost(X):\n",
    "    final = np.zeros(X.shape[0])\n",
    "    for t in range(T):\n",
    "        stump = DecisionTreeClassifier(max_depth=1)\n",
    "        stump.fit(X_train, y_train, sample_weight=weights)\n",
    "        final += alphas[t] * stump.predict(X)\n",
    "    return (final > 0).astype(int)\n",
    "\n",
    "y_pred_train = predict_boost(X_train)\n",
    "y_pred_test = predict_boost(X_test)\n",
    "\n",
    "print(\"\\n=== Manual AdaBoost Results ===\")\n",
    "print(\"Train Accuracy:\", accuracy_score(y_train, y_pred_train))\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred_test))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_test))\n",
    "\n",
    "# ----- Plots -----\n",
    "plt.plot(errors, marker='o')\n",
    "plt.title(\"Iteration vs Weighted Error\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(alphas, marker='o')\n",
    "plt.title(\"Iteration vs Alpha\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacff8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.6\n",
    ")\n",
    "\n",
    "ada.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n=== Sklearn AdaBoost ===\")\n",
    "print(\"Train Accuracy:\", ada.score(X_train, y_train))\n",
    "print(\"Test Accuracy:\", ada.score(X_test, y_test))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, ada.predict(X_test)))\n",
    "\n",
    "print(\"\\nComparison:\")\n",
    "print(\"Sklearn AdaBoost is more stable and usually more accurate than manual implementation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed82a40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_heart_disease\n",
    "data = load_heart_disease()\n",
    "\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "\n",
    "# split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "stump = DecisionTreeClassifier(max_depth=1)\n",
    "stump.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nWeak Stump — Heart Disease\")\n",
    "print(\"Train Accuracy:\", stump.score(X_train, y_train))\n",
    "print(\"Test Accuracy:\", stump.score(X_test, y_test))\n",
    "print(\"Confusion:\\n\", confusion_matrix(y_test, stump.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df0dc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators_list = [5,10,25,50,100]\n",
    "learning_rates = [0.1, 0.5, 1.0]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    results[lr] = []\n",
    "    for ne in n_estimators_list:\n",
    "        ada = AdaBoostClassifier(\n",
    "            base_estimator=DecisionTreeClassifier(max_depth=1),\n",
    "            n_estimators=ne,\n",
    "            learning_rate=lr\n",
    "        )\n",
    "        ada.fit(X_train, y_train)\n",
    "        acc = ada.score(X_test, y_test)\n",
    "        results[lr].append(acc)\n",
    "\n",
    "print(\"\\nHyperparameter Results:\")\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d1fbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lr = 1.0\n",
    "best_ne = 100\n",
    "\n",
    "ada_best = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=best_ne,\n",
    "    learning_rate=best_lr\n",
    ")\n",
    "ada_best.fit(X_train, y_train)\n",
    "\n",
    "errors = []\n",
    "sample_weights = []\n",
    "\n",
    "for est in ada_best.estimators_:\n",
    "    pred = est.predict(X_train)\n",
    "    err = np.mean(pred != y_train)\n",
    "    errors.append(err)\n",
    "\n",
    "plt.plot(errors, marker='o')\n",
    "plt.title(\"Weak Learner Error vs Iteration\")\n",
    "plt.show()\n",
    "\n",
    "# final weight distribution\n",
    "plt.hist(ada_best.estimator_weights_)\n",
    "plt.title(\"Final AdaBoost Sample Weights\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f80bc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = ada_best.feature_importances_\n",
    "feat_imp = pd.DataFrame({\n",
    "    \"feature\": X.columns,\n",
    "    \"importance\": importances\n",
    "}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "print(\"\\nTop 5 Important Features:\")\n",
    "print(feat_imp.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bee3f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# -------------------- Load Dataset --------------------\n",
    "file = \"WISDM_ar_v1.1_raw.txt\"\n",
    "\n",
    "cols = [\"user\", \"activity\", \"timestamp\", \"x\", \"y\", \"z\"]\n",
    "df = pd.read_csv(file, header=None, names=cols, delim_whitespace=True)\n",
    "\n",
    "# Keep only numeric accelerometer columns\n",
    "df = df[[\"activity\", \"x\", \"y\", \"z\"]]\n",
    "\n",
    "# Remove missing/dirty rows\n",
    "df = df.replace(\";\", np.nan)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "df[\"x\"] = df[\"x\"].astype(float)\n",
    "df[\"y\"] = df[\"y\"].astype(float)\n",
    "df[\"z\"] = df[\"z\"].astype(float)\n",
    "\n",
    "# -------------------- Binary Label: vigorous vs light --------------------\n",
    "vigorous = [\"Jogging\", \"Upstairs\"]\n",
    "light = [\"Walking\", \"Sitting\", \"Standing\", \"Downstairs\"]\n",
    "\n",
    "df[\"label\"] = df[\"activity\"].apply(lambda a: 1 if a in vigorous else 0)\n",
    "\n",
    "X = df[[\"x\", \"y\", \"z\"]]\n",
    "y = df[\"label\"]\n",
    "\n",
    "# Train-test split (70/30)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(df[\"label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c1fe57",
   "metadata": {},
   "outputs": [],
   "source": [
    "stump = DecisionTreeClassifier(max_depth=1)\n",
    "stump.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = stump.predict(X_train)\n",
    "y_pred_test = stump.predict(X_test)\n",
    "\n",
    "print(\"\\n=== Decision Stump Results ===\")\n",
    "print(\"Train Accuracy:\", accuracy_score(y_train, y_pred_train))\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred_test))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_test))\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"A stump can only split based on one threshold,\")\n",
    "print(\"So it struggles with complex continuous sensor data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fe4a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 20\n",
    "n = X_train.shape[0]\n",
    "\n",
    "weights = np.ones(n) / n\n",
    "\n",
    "alphas = []\n",
    "errors = []\n",
    "misclassified_history = []\n",
    "weight_history = []\n",
    "\n",
    "# Convert labels from {0,1} → {-1,+1} for Adaboost math\n",
    "y_train_mod = y_train.replace({0: -1, 1: 1})\n",
    "\n",
    "for t in range(T):\n",
    "\n",
    "    stump = DecisionTreeClassifier(max_depth=1)\n",
    "    stump.fit(X_train, y_train_mod, sample_weight=weights)\n",
    "\n",
    "    pred = stump.predict(X_train)\n",
    "\n",
    "    # weighted error\n",
    "    err = np.sum(weights * (pred != y_train_mod))\n",
    "    err = max(err, 1e-10)\n",
    "\n",
    "    alpha = 0.5 * np.log((1 - err) / err)\n",
    "\n",
    "    # Print required outputs\n",
    "    print(f\"\\n===== Round {t+1} =====\")\n",
    "    print(\"Weighted Error:\", err)\n",
    "    print(\"Alpha:\", alpha)\n",
    "\n",
    "    mc = np.where(pred != y_train_mod)[0]\n",
    "    print(\"Misclassified Indices:\", mc[:20])\n",
    "    print(\"Weights of Misclassified Samples:\", weights[mc][:10])\n",
    "\n",
    "    # Save history for plots\n",
    "    errors.append(err)\n",
    "    alphas.append(alpha)\n",
    "    misclassified_history.append(mc)\n",
    "    weight_history.append(weights.copy())\n",
    "\n",
    "    # Update weights\n",
    "    weights = weights * np.exp(-alpha * y_train_mod * pred)\n",
    "    weights = weights / np.sum(weights)\n",
    "\n",
    "# ---------- Final strong classifier ----------\n",
    "def predict_manual_boost(X):\n",
    "    final_pred = np.zeros(X.shape[0])\n",
    "    for t in range(T):\n",
    "        stump = DecisionTreeClassifier(max_depth=1)\n",
    "        stump.fit(X_train, y_train_mod, sample_weight=weight_history[t])\n",
    "        final_pred += alphas[t] * stump.predict(X)\n",
    "    return (final_pred > 0).astype(int)\n",
    "\n",
    "y_pred_train = predict_manual_boost(X_train)\n",
    "y_pred_test = predict_manual_boost(X_test)\n",
    "\n",
    "print(\"\\n=== Manual AdaBoost Final Results ===\")\n",
    "print(\"Train Accuracy:\", accuracy_score(y_train, y_pred_train))\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred_test))\n",
    "print(\"Confusion:\\n\", confusion_matrix(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257f68f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=100,\n",
    "    learning_rate=1.0\n",
    ")\n",
    "\n",
    "ada.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n=== Sklearn AdaBoost ===\")\n",
    "print(\"Train Accuracy:\", ada.score(X_train, y_train))\n",
    "print(\"Test Accuracy:\", ada.score(X_test, y_test))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, ada.predict(X_test)))\n",
    "\n",
    "print(\"\\nComparison:\")\n",
    "print(\"Sklearn AdaBoost is more stable because:\")\n",
    "print(\"✔ Perfect weight normalization\")\n",
    "print(\"✔ Better stump-based sampling\")\n",
    "print(\"✔ Optimized implementation\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
